## MMVID<br><sub>Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning (CVPR 2022)</sub>

### [Project](https://snap-research.github.io/MMVID/)

<div align="center">
  Generated Videos on Multimodal VoxCeleb
</div>

<div class="gif">
<p align="center">
<img src='images/demo.gif' align="center" width=400>
</p>
</div>

This repo will contain the code for training and testing, models, and data for MMVID (coming soon).

> [**Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning**](https://xxx)<br>
> [Ligong Han](https://phymhan.github.io/), [Jian Ren](https://alanspike.github.io/), [Hsin-Ying Lee](http://hsinyinglee.com/), [Francesco Barbieri](https://fvancesco.github.io/), [Kyle Olszewski](https://kyleolsz.github.io/), [Shervin Minaee](https://sites.google.com/site/shervinminaee/home), [Dimitris Metaxas](https://people.cs.rutgers.edu/~dnm/), [Sergey Tulyakov](http://www.stulyakov.com/)<br>
> Snap Inc., Rutgers University<br>
> CVPR 2022


## Citation

If our code, data, or models help your work, please cite our paper:
```
@inproceedings{han2022mmvid,
    title={Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning},
    author={Ligong Han and Jian Ren and Hsin-Ying Lee and Francesco Barbieri and Kyle Olszewski and Shervin Minaee and Dimitris Metaxas and Sergey Tulyakov},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2022}
}
```
